{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Interpretability via Attentional and Memory-based Interfaces Using TensorFlow**\n",
    "#### A closer look at the reasoning inside your deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**TLDR**: This post will serve as a gentle introduction to attentional and memory-based interfaces in deep neural architectures using TensorFlow. Incorporation of attention mechanisms is very simple and can improve transparency and interpretability in our complex models. We will conclude with extensions and caveats of the interfaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### [Just take me to the model!](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### **Table of Contents**\n",
    "1. [Introduction](#1)\n",
    "2. [Overview of Attentional Interfaces](#2)\n",
    "3. [Basic Sentiment Analysis](#3)\n",
    "4. [Set Up](#4)\n",
    "5. [Preprocessing Components](#5)\n",
    "6. [Sample the Data](#6)\n",
    "7. [Model](#7)\n",
    "7. [Training](#8)\n",
    "9. [Results](#9)\n",
    "10. [Attention for a Sample](#10)\n",
    "11. [Attentional History](#11)\n",
    "12. [Attentional Interface Variants](#12)\n",
    "13. [Caveats](#13)\n",
    "13. [Conclusion](#14)\n",
    "14. [References](#15)\n",
    "15. [Author Bio](#16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='1'></a>\n",
    "### **I. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attentional interfaces in deep neural networks are loosely based on visual attention mechanisms in many animals [1]. These mechanisms allow the organisms to dynamically focus on pertinent parts of a visual input and respond accordingly. This basic idea of selective attention has been carried over to deep learning where it is being used in image analysis [2], translation [3], question answering [4], speech [5], and a variety of other tasks.\n",
    "\n",
    "![](images/image_attn.jpg)\n",
    "<center><b>Figure 1</b> Attention on an image for a specific (underlined) word in the caption.</center>\n",
    "<center>[Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel: “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”.](http://arxiv.org/abs/1502.03044)</center>\n",
    "\n",
    "\n",
    "These interfaces also offer much needed model interpretability by allowing us to see which parts of the input are attended to at any point in time. A common disadvantage with deep neural architectures is the lack of interpretability and the associated \"black box\" stigma. Implementation of these interfaces has not only been shown to increase model performance but also offer more transparent and sensible results. And as you will see in our implementation below, they produce some pretty cool visualizations that are consistent with how humans would naturally attend to the inputs.\n",
    "\n",
    "![](images/nlu_attn.jpg)\n",
    "<center><b>Figure 2</b> Attention heatmaps offering interpretability on where the model is looking to respond.</center>\n",
    "<center>[Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman: “Teaching Machines to Read and Comprehend”.](http://arxiv.org/abs/1506.03340)</center>\n",
    "\n",
    "Research on attentional interfaces is very popular these days, as they offer multiple benefits. There are increasingly complex variants to the attention mechanisms but the overall foundation remains the same. In this post, we will take a look at the fundamental attentional interface, implement it into a small model and then discuss some recent variants. We will be using attention in Natural Language Understanding (NLU) tasks but will briefly explore other areas as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='2'></a>\n",
    "### **II. Overview of Attentional Interfaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's take a closer look at the fundamental idea behind an attentional interface. The attention model takes in $j$ inputs ($h_1, h_2, … h_j$), along with some information $s_{i-1}$ and outputs a vector $c$ which is the summary of $h_i$ focusing on the information $s_{i-1}$. Before we see what goes on internally, it is important to point out that there are a couple different options for attention (soft, hard) but the type we will focus on will all be completely differentiable (ex. soft). This is because we want to be able to learn _where to focus_. This also means that we will also be focusing everywhere at all times but we will learn _where to place more attention_.\n",
    "\n",
    "We can think of our $s_{i-1}$ as the generated context describing what we should focus on. We take the dot product of each input item with this context in order to produce a score. The scores ($e$ in the equations in fig. 3) are fed into a softmax activation function to create our attention distribution. We multiply these normalized scores with our original inputs to receive a weighted summary vector for timestep/input $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](images/soft.jpg)\n",
    "<br><br>\n",
    "<center><b>Figure 3 </b>Internal operations for making the summariazing vector c.</center>\n",
    "<center>Credit: Goku Mohandas</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the main advantages of using an attentional interface such as this is the opportunity to interpret and visualize the _attention scores_. We can see for each input where the model places more attention, and how that impacts our final result. There are many ways to implement fully differential attentional interfaces, and the internal intricacies can differ. The example we have above is very commonly seen in _neural translation models_.\n",
    "\n",
    "With translation, we don’t want just one summarizing vector using all of the inputs; we want a summarizing vector for each input. This is because translation is not always a one-to-one output. Words in one language may result in multiple words in another, so we need to attend to the entire sentence for all inputs, while using that context information so we know what we have seen so far.\n",
    "\n",
    "We can go ahead and plot the attention scores for translation, and we will see something like Figure 4, which maps between English and French. Notice how some words, like \"me\", very clearly only need to attend to one word. But other words, or groups of words, require attention across multiple words from the source language in order to translate properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](images/nmt.jpg)\n",
    "<br><br>\n",
    "<center><b>Figure 4 </b>Architecture and attention visualization for a basic neural machine translation task, such as translating between English and French.</center>\n",
    "<center>Credit: Goku Mohandas</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='3'></a>\n",
    "### **III. Basic Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will be using a sentiment analysis task in order to implement a basic attentional interface. This simple use case will be a nice introduction to how we can add attention to existing models, for improved performance and interpretability. \n",
    "\n",
    "We will be using the [Large Movie Review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which contains 50,000 reviews that are split evenly into 25k train and 25k test sets. The dataset includes text reviews with ratings from 0 through 9, but we will use ratings 0-4 as positive and 5-9 as negative sentiment. More specifics, including unlabeled data for unsupervised learning, and data collection details are provided in the dataset's README.md.\n",
    "\n",
    "Here is the overview architecture including the attentional interface. We will cover the details of preprocessing, the model, and the training procedure below. In Figure 5, you will notice how the attention layer is almost just like a pluggable interface. We could have just as easily taken the last (relevant) hidden state from the encoder and applied a nonlinearity, followed by normalization, to receive a predicted sentiment. Adding this attentional layer, however, gives us the chance to look inside the model, and gain interpretability from the inferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/architecture.jpg\" width=\"700\">\n",
    "<br><br>\n",
    "<center><b>Figure 5 </b>Model architecture overview.</center>\n",
    "<center>Credit: Goku Mohandas</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='4'></a>\n",
    "### **IV. Setup**\n",
    "Before running the Jupyter notebook [attention.ipynb](attention.ipynb), you'll need to ensure that your machine is set up to run TensorFlow and install a handful of helpful additional libraries.\n",
    "\n",
    "1. Download [this entire repo from GitHub](https://github.com/ajarai/O-Reilly)\n",
    "\n",
    "2. Open your terminal and use `cd` to navigate into the top directory of the repo on your machine\n",
    "\n",
    "3. Unzip the processed reviews data by entering\n",
    "```bash\n",
    "unzip ./attention/processed_reviews.zip\n",
    "```\n",
    "\n",
    "Now, set up your environment by using the Dockerfile included with this repo (option A) or configuring it manually (option B).\n",
    "\n",
    "#### Option A: use the Dockerfile configured for this notebook\n",
    "(Note: For GPU versions, etc. check out [TensorFlow's Docker repo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker).)\n",
    "\n",
    "5. After downloading this repo to your machine, open your terminal and use `cd` to navigate to the directory that contains `Dockerfile.cpu`.\n",
    "\n",
    "6. To build the Dockerfile, enter\n",
    "```bash\n",
    "docker build -t dockerfile_cpu -f dockerfile.cpu .\n",
    "```\n",
    "If you get a permissions error on running this command, you may need to run it with `sudo`:\n",
    "```bash\n",
    "sudo  build -t dockerfile_cpu -f dockerfile.cpu .\n",
    "```\n",
    "\n",
    "7. Run Docker from the Dockerfile you've just built\n",
    "```bash\n",
    "docker run -it -p 8888:8888 -p 6006:6006 dockerfile_cpu bash\n",
    "```\n",
    "or\n",
    "```bash\n",
    "sudo docker run -it -p 8888:8888 -p 6006:6006 dockerfile_cpu bash\n",
    "```\n",
    "if you run into permission problems.\n",
    "\n",
    "8. Launch Jupyter by entering\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "and, using your browser, navigate to the URL shown in the terminal output (usually http://localhost:8888/)\n",
    "\n",
    "#### Option B: set up environment manually on local machine\n",
    "For instructions on installing TensorFlow, please see [TensorFlow's platform-specific installation instructions](https://www.tensorflow.org/install/).\n",
    "```bash\n",
    "Required:\n",
    "    python 3.3+\n",
    "    pip 9.0.1 (sudo easy_install pip)\n",
    "    virtualenv (pip install virtualenv)\n",
    "Optional: CUDA enabled GPU (explicity define cpu components (embedding, etc.)\n",
    "```\n",
    "\n",
    "5. Enter the `attention` directory in your terminal and enter the following commands\n",
    "```bash\n",
    "make venv\n",
    "source oreilly-attention-venv/bin/activate\n",
    "make load-data\n",
    "ipython3 notebook attention\n",
    "```\n",
    "6. Launch Jupyter by entering\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "and, using your browser, navigate to the URL shown in the terminal output (usually http://localhost:8888/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Establish basedir (useful if running as python package)\n",
    "import os\n",
    "basedir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hide all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='5'></a>\n",
    "### **V. Preprocessing Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section, we will preprocess our raw input data. The main components are the Vocab class which we initialize using our `vocab.txt` file. This file contains all of the tokens (words) from our raw input, sorted by descending frequency. The next helper function we need is `ids_to_tokens()`, which will convert a list of ids into tokens we can understand. We will use this for reading our input and associating the word with its respective attention score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11,
     15,
     19,
     23
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess the reviews.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from unidecode import (\n",
    "    unidecode,\n",
    ")\n",
    "\n",
    "from random import (\n",
    "    shuffle,\n",
    ")\n",
    "\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    ")\n",
    "\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     63
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    \"\"\"\n",
    "    Class for processing tokens to ids and vice versa.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file, max_vocab_size=200000, verbose=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self._token_to_id = {}\n",
    "        self._id_to_token = {}\n",
    "        self._size = -1\n",
    "\n",
    "        with open(vocab_file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "\n",
    "                # White space in vocab file (' ': <count>)\n",
    "                if len(tokens) == 1:\n",
    "                    count = tokens[0]\n",
    "                    idx = line.index(count)\n",
    "                    t = line[:idx-1]\n",
    "                    tokens = (t, count)\n",
    "\n",
    "                if len(tokens) != 2:\n",
    "                    continue\n",
    "\n",
    "                if tokens[0] in self._token_to_id:\n",
    "                    continue\n",
    "\n",
    "                self._size += 1\n",
    "                if self._size > max_vocab_size:\n",
    "                    print ('Too many tokens! >%i/n' % max_vocab_size)\n",
    "                    break\n",
    "\n",
    "                self._token_to_id[tokens[0]] = self._size\n",
    "                self._id_to_token[self._size] = tokens[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return vocabulary size.\n",
    "        \"\"\"\n",
    "        return self._size+1\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        Return the corresponding id for a token.\n",
    "        \"\"\"\n",
    "        if token not in self._token_to_id:\n",
    "            if self.verbose:\n",
    "                print (\"ID not found for %s\" % token)\n",
    "            return self._token_to_id[UNKNOWN_TOKEN]\n",
    "        return self._token_to_id[token]\n",
    "\n",
    "    def id_to_token(self, _id):\n",
    "        \"\"\"\n",
    "        Returnn the correspoding token for an id.\n",
    "        \"\"\"\n",
    "        if _id not in self._id_to_token:\n",
    "            if self.verbose:\n",
    "                print (\"Token not found for ID: %i\" % _id)\n",
    "            return UNKNOWN_TOKEN\n",
    "        return self._id_to_token[_id]\n",
    "\n",
    "def ids_to_tokens(ids_list, vocab):\n",
    "    \"\"\"\n",
    "    Convert a list of ids to tokens.\n",
    "    Args:\n",
    "        ids_list: list of ids to convert to tokens.\n",
    "        vocab: Vocab class object.\n",
    "    Returns:\n",
    "        answer: list of tokens that corresponds to ids_list.\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    for _id in ids_list:\n",
    "        token = vocab.id_to_token(_id)\n",
    "        if token == PAD_TOKEN:\n",
    "            continue\n",
    "        answer.append(token)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='6'></a>\n",
    "### **VI. Sample the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section, we will see what our inputs will look like. The `processed_review` represents our reviews with ids. The `review_seq_len` tells us how long the review is. Unless we use dynamic computation graphs, we need to feed in fixed sized inputs into our TensorFlow models per batch. This means that we will have some PADs and we do not want these to influence our model. In this implementation, the PADs do not prove to be too problematic since inference will depend on the entire summarized context (so no loss masking needed). And we also want to keep them, even when determining the attention scores, to show how the model learns not to focus on the PADs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"  \n",
    "        self.data_dir=\"data/processed_reviews/train.p\"           # location of reviews data (train|validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_data(data_path):\n",
    "    \"\"\"\n",
    "    Sample format of the processed\n",
    "    data from data.py\n",
    "    Args:\n",
    "        data_path: path for train.p|valid.p\n",
    "    \"\"\"\n",
    "    with open(data_path, 'rb') as f:\n",
    "        entries = pickle.load(f)\n",
    "\n",
    "    # Choose a random sample\n",
    "    rand_index = random.randint(0, len(entries))\n",
    "\n",
    "    # Prepare vocab\n",
    "    vocab_file = os.path.join(basedir, 'data/processed_reviews/vocab.txt')\n",
    "    vocab = Vocab(vocab_file, verbose=False)\n",
    "\n",
    "    # Sample\n",
    "    (processed_review,\n",
    "     review_seq_len,\n",
    "     label) = entries[rand_index]\n",
    "\n",
    "    print (\"==> Processed Review:\", processed_review)\n",
    "    print (\"==> Review Len:\", review_seq_len)\n",
    "    print (\"==> Label:\", label)\n",
    "    print (\"==> See if processed review makes sense:\",\n",
    "        ids_to_tokens(\n",
    "            processed_review,\n",
    "            vocab=vocab,\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "sample_data(FLAGS.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='7'></a>\n",
    "### **VII. Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will start by talking about operation functions. `_xavier_weight_init()` is a little function we made to properly initialize our weights depending on the nonlinearity that will be applied to them. The initialization is such that we will receive outputs with unit variance prior to sending to the activation function. This is an optimization technique so we do not have large values when applying the nonlinearity, as that will lead to saturation at the extremes, leading to gradient issues. We also have a helper function for layer normalization, `ln()`, which is another optimization technique that will normalize our inputs into the GRU before applying the activation function. This will allow us to control gradient issues and even allow us to use larger learning rates. The layer normalization is applied in the `customGRU()` function prior to the sigmoid and tanh operations. The last helper function is `add_dropout_and_layers()` which will add dropout to our recurrent outputs and will allow us to create multi-layered recurrent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Operation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     10,
     15,
     27,
     32,
     36,
     62,
     107,
     127,
     165
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Operation functions for model.py\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import (\n",
    "    utils,\n",
    ")\n",
    "\n",
    "from tensorflow.python.framework import (\n",
    "    ops,\n",
    "    tensor_shape,\n",
    ")\n",
    "\n",
    "from tensorflow.python.ops import (\n",
    "        gen_array_ops,\n",
    "        array_ops,\n",
    "        clip_ops,\n",
    "        embedding_ops,\n",
    "        init_ops,\n",
    "        math_ops,\n",
    "        nn_ops,\n",
    "        partitioned_variables,\n",
    "        variable_scope as vs,\n",
    ")\n",
    "\n",
    "from tensorflow.python.ops.math_ops import (\n",
    "        sigmoid,\n",
    "        tanh,\n",
    ")\n",
    "\n",
    "from tensorflow.python.util import (\n",
    "    nest,\n",
    ")\n",
    "\n",
    "def _xavier_weight_init(nonlinearity='tanh'):\n",
    "    \"\"\"\n",
    "    Xavier weights initialization.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"\n",
    "        Tanh and sigmoid initialization.\n",
    "        \"\"\"\n",
    "        eps = 1.0 / np.sqrt(np.sum(shape))\n",
    "        return tf.random_uniform(shape, minval=-eps, maxval=eps)\n",
    "\n",
    "    def _relu_xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"\n",
    "        ReLU initialization.\n",
    "        \"\"\"\n",
    "        eps = np.sqrt(2.0) / np.sqrt(np.sum(shape))\n",
    "        return tf.random_uniform(shape, minval=-eps, maxval=eps)\n",
    "\n",
    "    if nonlinearity in ('tanh', 'sigmoid'):\n",
    "        return _xavier_initializer\n",
    "    elif nonlinearity in ('relu'):\n",
    "        return _relu_xavier_initializer\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Please choose a valid nonlinearity: tanh|sigmoid|relu\")\n",
    "\n",
    "def _linear(args, output_size, bias, bias_start=0.0,\n",
    "    nonlinearity='relu', scope=None, name=None):\n",
    "    \"\"\"\n",
    "    Sending inputs through a two layer MLP.\n",
    "    Args:\n",
    "        args: list of inputs of shape (N, H)\n",
    "        output_size: second dimension of W\n",
    "        bias: boolean, whether or not to add bias\n",
    "        bias_start: initial bias value\n",
    "        nonlinearity: nonlinear transformation to use (tanh|sigmoid|relu)\n",
    "        scope: (optional) Variable scope to create parameters in.\n",
    "        name: (optional) variable name.\n",
    "    Returns:\n",
    "        Tensor with shape (N, output_size)\n",
    "    \"\"\"\n",
    "    _input = tf.concat(\n",
    "        values=args,\n",
    "        axis=1,)\n",
    "    shape = _input.get_shape()\n",
    "    # Computation\n",
    "    scope = vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        w_name = \"W_1_\"\n",
    "        if name is not None:\n",
    "            w_name += name\n",
    "        W_1 = vs.get_variable(\n",
    "            name=w_name,\n",
    "            shape=[shape[1], output_size],\n",
    "            initializer=_xavier_weight_init(\n",
    "                nonlinearity=nonlinearity),\n",
    "            )\n",
    "        result_1 = tf.matmul(_input, W_1)\n",
    "        if bias:\n",
    "            b_name = \"b_1_\"\n",
    "            if name is not None:\n",
    "                b_name += name\n",
    "            b_1 = vs.get_variable(\n",
    "                name=b_name,\n",
    "                shape=(output_size,),\n",
    "                initializer=init_ops.constant_initializer(\n",
    "                    bias_start, dtype=tf.float32),\n",
    "                )\n",
    "            result_1 = tf.add(result_1, b_1)\n",
    "    return result_1\n",
    "\n",
    "def ln(inputs, epsilon=1e-5, scope=None):\n",
    "\n",
    "    \"\"\" Computer layer norm given an input tensor. We get in an input of shape\n",
    "    [N X D] and with LN we compute the mean and var for each individual\n",
    "    training point across all it's hidden dimensions rather than across\n",
    "    the training batch as we do in BN. This gives us a mean and var of shape\n",
    "    [N X 1].\n",
    "    \"\"\"\n",
    "    mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n",
    "    with tf.variable_scope(scope + 'LN'):\n",
    "            scale = tf.get_variable('alpha',\n",
    "                shape=[inputs.get_shape()[1]],\n",
    "                initializer=tf.constant_initializer(1))\n",
    "            shift = tf.get_variable('beta',\n",
    "                shape=[inputs.get_shape()[1]],\n",
    "                initializer=tf.constant_initializer(0))\n",
    "    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift\n",
    "\n",
    "    return LN\n",
    "\n",
    "class custom_GRUCell(tf.contrib.rnn.RNNCell):\n",
    "        \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n",
    "\n",
    "        def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "            if input_size is not None:\n",
    "                logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "            self._num_units = num_units\n",
    "            self._activation = activation\n",
    "\n",
    "        @property\n",
    "        def state_size(self):\n",
    "            return self._num_units\n",
    "\n",
    "        @property\n",
    "        def output_size(self):\n",
    "            return self._num_units\n",
    "\n",
    "        def __call__(self, inputs, state, scope=None):\n",
    "            \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "            with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n",
    "                with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
    "                    # We start with bias of 1.0 to not reset and not update.\n",
    "                    r, u = array_ops.split(\n",
    "                            _linear([inputs, state], 2 * self._num_units, True, 1.0), 2, 1,\n",
    "                    )\n",
    "\n",
    "                    # Apply Layer Normalization to the two gates\n",
    "                    r = ln(r, scope = 'r/')\n",
    "                    u = ln(r, scope = 'u/')\n",
    "\n",
    "                    r, u = sigmoid(r), sigmoid(u)\n",
    "                with vs.variable_scope(\"Candidate\"):\n",
    "                    c = self._activation(\n",
    "                        _linear([inputs, r * state],\n",
    "                            self._num_units, True))\n",
    "                new_h = u * state + (1 - u) * c\n",
    "            return new_h, new_h\n",
    "\n",
    "def add_dropout_and_layers(single_cell, keep_prob, num_layers):\n",
    "    \"\"\"\n",
    "    Add dropout and create stacked layers using a single_cell.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropout\n",
    "    stacked_cell = tf.contrib.rnn.DropoutWrapper(single_cell,\n",
    "        output_keep_prob=keep_prob)\n",
    "\n",
    "    # Each state as one cell\n",
    "    if num_layers > 1:\n",
    "        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [single_cell] * num_layers)\n",
    "\n",
    "    return stacked_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's briefly describe the model pipelines and see how our inputs undergo representation changes. First we will initialize our placeholders which will hold the reviews, lens, sentiment, embeddings etc. Then we will build the encoder which will take our input review and first embed using the GloVe embeddings. We will then feed the embedded tokens into a GRU in order to encode the input. We will use the output from each timestep in the GRU as our inputs to the attentional layer. Notice that we could have completely removed the attentional interface and just use the last relevant hidden state from the encoder GRU in order to receive our predicted sentiment. But adding this attention layer allows us to see how the model processes the input review.\n",
    "\n",
    "In the attentional layer, we apply a nonlinearity followed by another one, in order to reduce to dimension 1. Now, we can normalize to compute our attention scores. These scores are then broadcasted and multiplied with the original inputs to receive our summarized vector. We use this vector to receive our predicted sentiment via normalization in the decoder. Notice that we do not use a previous state ($s_{i-1}$) since the task involves creating just one context and extracting the sentiment from that.\n",
    "\n",
    "We then define our loss as the cross entropy between the predicted and the ground truth sentiment. We use a bit of decay for our learning rate with an absolute minimum and use the ADAM optimizer [9]. With all of these components, we have built our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     10,
     16,
     38,
     57,
     75,
     108,
     175,
     254,
     275,
     287,
     312
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple GRU Encoder/Decoder Model w/ Attentional Interface\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    \"\"\"\n",
    "    Tensorflow graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, FLAGS, vocab_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.FLAGS = FLAGS\n",
    "        self._vsize = vocab_size\n",
    "\n",
    "    def train(self, sess, batch_reviews, batch_labels,\n",
    "        batch_review_lens, embeddings, keep_prob):\n",
    "        \"\"\"\n",
    "        Train the model using a batch and predicted guesses.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._train_op,\n",
    "            self._logits,\n",
    "            self._loss,\n",
    "            self._accuracy,\n",
    "            self._lr,\n",
    "            self._Z,\n",
    "                   ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._labels: batch_labels,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "                  }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def eval(self, sess, batch_reviews, batch_labels,\n",
    "        batch_review_lens, embeddings, keep_prob=1.0):\n",
    "        \"\"\"\n",
    "        Evaluation of validation set.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._logits,\n",
    "            self._loss,\n",
    "            self._accuracy,\n",
    "        ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._labels: batch_labels,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "            }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def infer(self, sess, batch_reviews,\n",
    "        batch_review_lens, embeddings, keep_prob=1.0):\n",
    "        \"\"\"\n",
    "        Inference with a sample sentence.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._logits,\n",
    "            self._probabilities,\n",
    "            self._Z,\n",
    "        ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "            }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def _add_placeholders(self):\n",
    "        \"\"\"\n",
    "        Input that will be fed into our DCN graph.\n",
    "        \"\"\"\n",
    "        print (\"==> Adding placeholders:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        self._reviews = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None, FLAGS.max_input_length],\n",
    "            name=\"reviews\")\n",
    "        self._review_lens = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None, ],\n",
    "            name=\"review_lens\")\n",
    "        self._labels = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None,],\n",
    "            name=\"labels\")\n",
    "        self._embeddings = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=(FLAGS.vocab_size, FLAGS.emb_size),\n",
    "            name='glove_embeddings')\n",
    "        self._keep_prob = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=(),\n",
    "            name=\"keep_prob\")\n",
    "\n",
    "        print (\"\\t self._reviews:\", self._reviews.get_shape())\n",
    "        print (\"\\t self._labels:\", self._labels.get_shape())\n",
    "        print (\"\\t self._embeddings:\", self._embeddings.get_shape())\n",
    "        print (\"\\t self._keep_prob:\", self._keep_prob.get_shape())\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        \"\"\"\n",
    "        Constructing the encoder.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the encoder:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        batch_size = FLAGS.batch_size\n",
    "        hidden_size = FLAGS.hidden_size\n",
    "        max_input_length = FLAGS.max_input_length\n",
    "\n",
    "        with tf.variable_scope('embedding'):\n",
    "            print (\"\\t embedding:\")\n",
    "\n",
    "            if FLAGS.embedding == 'random':\n",
    "                # Random embedding weights\n",
    "                embedding = tf.get_variable(\n",
    "                    name='embedding',\n",
    "                    shape=[self._vsize, FLAGS.emb_size],\n",
    "                    dtype=tf.float32,\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=1e-4),\n",
    "                    trainable=FLAGS.train_embedding)\n",
    "            elif FLAGS.embedding == 'glove':\n",
    "                # GloVe embedding weights\n",
    "                embedding = self._embeddings\n",
    "\n",
    "            # Check embedding dim\n",
    "            if embedding.get_shape()[1] != FLAGS.emb_size:\n",
    "                raise Exception(\n",
    "                    \"Embedding's dimension does not match specified emb_size.\")\n",
    "\n",
    "            # Embedding the review\n",
    "            fn = lambda x: tf.nn.embedding_lookup(embedding, x)\n",
    "            c_embedding = tf.map_fn(\n",
    "                lambda x: fn(x), self._reviews, dtype=tf.float32)\n",
    "\n",
    "            print (\"\\t\\t embedding:\", embedding.get_shape())\n",
    "            print (\"\\t\\t reviews_embedded:\", c_embedding.get_shape())\n",
    "\n",
    "        with tf.variable_scope('c_encoding'):\n",
    "            print (\"\\t c_encoding:\")\n",
    "\n",
    "            # GRU cells\n",
    "            cell = add_dropout_and_layers(\n",
    "                single_cell=custom_GRUCell(hidden_size),\n",
    "                keep_prob=self._keep_prob,\n",
    "                num_layers=FLAGS.num_layers,\n",
    "                )\n",
    "\n",
    "            # Dynamic-GRU\n",
    "            # return (outputs, last_output_states (relevant))\n",
    "            all_outputs, h = tf.nn.dynamic_rnn(\n",
    "                cell=cell,\n",
    "                inputs=c_embedding,\n",
    "                #sequence_length=self._review_lens,\n",
    "                dtype=tf.float32,\n",
    "                time_major=False,\n",
    "                )\n",
    "\n",
    "            self._all_outputs = all_outputs\n",
    "            self._h = h\n",
    "\n",
    "            self._z = all_outputs\n",
    "\n",
    "            print (\"\\t\\t self._all_outputs\", self._all_outputs.get_shape())\n",
    "            print (\"\\t\\t self._h\", self._h.get_shape())\n",
    "\n",
    "    def _build_attentional_interface(self):\n",
    "        \"\"\"\n",
    "        Adding an attentional interface\n",
    "        for model interpretability.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the attentional interface:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        batch_size = FLAGS.batch_size\n",
    "        hidden_size = FLAGS.hidden_size\n",
    "        max_input_length = FLAGS.max_input_length\n",
    "        loop_until = tf.to_int32(np.array(range(batch_size)))\n",
    "\n",
    "        with tf.variable_scope('attention') as attn_scope:\n",
    "            print (\"\\t attention:\")\n",
    "\n",
    "            # Time-major self._all_outputs (N, M, H) --> (M, N, H)\n",
    "            all_outputs_time_major = tf.transpose(self._all_outputs,\n",
    "                perm=[1,0,2])\n",
    "\n",
    "            # Apply tanh nonlinearity\n",
    "            fn = lambda _input: tf.nn.tanh(_linear(\n",
    "                    args=_input,\n",
    "                    output_size=hidden_size,\n",
    "                    bias=True,\n",
    "                    bias_start=0.0,\n",
    "                    nonlinearity='tanh',\n",
    "                    scope=attn_scope,\n",
    "                    name='attn_nonlinearity',\n",
    "                    ))\n",
    "            z = tf.map_fn(\n",
    "                lambda x: fn(x), all_outputs_time_major, dtype=tf.float32)\n",
    "\n",
    "            # Apply softmax weights\n",
    "            fn = lambda _input: tf.nn.tanh(_linear(\n",
    "                    args=_input,\n",
    "                    output_size=1,\n",
    "                    bias=True,\n",
    "                    bias_start=0.0,\n",
    "                    nonlinearity='tanh',\n",
    "                    scope=attn_scope,\n",
    "                    name='attn_softmax',\n",
    "                    ))\n",
    "            z = tf.map_fn(\n",
    "                lambda x: fn(x), z, dtype=tf.float32)\n",
    "\n",
    "            # Squeeze and convert to batch major\n",
    "            z = tf.transpose(\n",
    "                    tf.squeeze(\n",
    "                        input=z,\n",
    "                        axis=2,\n",
    "                        ),\n",
    "                    perm=[1,0])\n",
    "\n",
    "            # Normalize\n",
    "            self._Z = tf.nn.softmax(\n",
    "                logits=z,\n",
    "                )\n",
    "\n",
    "            # Create context vector (via soft attention.)\n",
    "            fn = lambda sample_num: \\\n",
    "                tf.reduce_sum(\n",
    "                    tf.multiply(\n",
    "                        self._all_outputs[sample_num][:self._review_lens[sample_num]],\n",
    "\n",
    "                        # (500,) --> (500, 1) --> (500, 200)\n",
    "                        tf.tile(\n",
    "                            input=tf.expand_dims(\n",
    "                                self._Z[sample_num][:self._review_lens[sample_num]], 1),\n",
    "                            multiples=(1, hidden_size),\n",
    "                        )),\n",
    "                    axis=0)\n",
    "\n",
    "            self._c = tf.map_fn(\n",
    "                lambda sample_num: fn(sample_num), loop_until, dtype=tf.float32)\n",
    "\n",
    "            print (\"\\t\\t self._Z\", self._Z.get_shape())\n",
    "            print (\"\\t\\t self._c\", self._c.get_shape())\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        \"\"\"\n",
    "        Applying a softmax on output of encoder.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the decoder:\")\n",
    "        with tf.variable_scope('softmax'):\n",
    "            print (\"\\t Softmax:\")\n",
    "            self._logits = _linear(\n",
    "                args=self._c, # self._c (with attn) or self._h (no attn)\n",
    "                output_size=self.FLAGS.num_classes,\n",
    "                bias=True,\n",
    "                bias_start=0.0,\n",
    "                nonlinearity='relu',\n",
    "                name='softmax_op',\n",
    "                )\n",
    "            self._probabilities = tf.nn.softmax(\n",
    "                logits=self._logits,\n",
    "                )\n",
    "            print (\"\\t\\t self._logits\", self._logits.get_shape())\n",
    "            print (\"\\t\\t self._probabilities\", self._probabilities.get_shape())\n",
    "\n",
    "    def _add_loss(self):\n",
    "        \"\"\"\n",
    "        Determine the loss.\n",
    "        \"\"\"\n",
    "        print (\"==> Establishing the loss function.\")\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self._labels, logits=self._logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self._logits, 1),\n",
    "            tf.cast(self._labels, tf.int64)), tf.float32))\n",
    "        return self.loss, self.accuracy\n",
    "\n",
    "    def _add_train_op(self):\n",
    "        \"\"\"\n",
    "        Add the training optimizer.\n",
    "        \"\"\"\n",
    "        print (\"==> Creating the training optimizer.\")\n",
    "\n",
    "        # Decay learning rate\n",
    "        self._lr = tf.maximum(\n",
    "            self.FLAGS.min_lr,\n",
    "            tf.train.exponential_decay(\n",
    "                learning_rate=self.FLAGS.lr,\n",
    "                global_step=self.global_step,\n",
    "                decay_steps=100000,\n",
    "                decay_rate=self.FLAGS.decay_rate,\n",
    "                staircase=False,\n",
    "                ))\n",
    "\n",
    "        # Training releaved no clipping needed\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self._lr).minimize(self.loss,\n",
    "            global_step=self.global_step)\n",
    "        return self.optimizer\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Contrust each component of the TF graph.\n",
    "        \"\"\"\n",
    "        self._add_placeholders()\n",
    "        self._build_encoder()\n",
    "        self._build_attentional_interface()\n",
    "        self._build_decoder()\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False) # won't step\n",
    "        if self.FLAGS.mode == 'train':\n",
    "            self._loss, self._accuracy = self._add_loss()\n",
    "            self._train_op = self._add_train_op()\n",
    "\n",
    "        # Components for model saving\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        print (\"==> Review Classifier built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='8'></a>\n",
    "### **VIII. Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_epoch(data_path, num_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Generate num_epoch epochs.\n",
    "    Args:\n",
    "        data_path: path for train.p|valid.p\n",
    "        num_epochs: number of epochs to run for\n",
    "        batch_size: samples per each batch\n",
    "    \"\"\"\n",
    "    with open(data_path, 'rb') as f:\n",
    "        entries = pickle.load(f)\n",
    "\n",
    "    processed_contexts, processed_answers = [], []\n",
    "    context_lens = []\n",
    "\n",
    "    for entry in entries:\n",
    "        processed_contexts.append(entry[0])\n",
    "        context_lens.append(entry[1])\n",
    "        processed_answers.append(entry[2])\n",
    "\n",
    "    features = [processed_contexts, processed_answers]\n",
    "    seq_lens = [context_lens,]\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(features, seq_lens, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(features, seq_lens, batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches of size <batch_size>.\n",
    "    Args:\n",
    "        features: processed contexts, questions and answers.\n",
    "        seq_lens: context and question actual (pre-padding) seq-lens.\n",
    "        batch_size: samples per each batch\n",
    "    \"\"\"\n",
    "    data_size = len(features[0])\n",
    "    num_batches = data_size//batch_size\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num+1)*batch_size, data_size)\n",
    "\n",
    "        batch_features = []\n",
    "        for feature in features:\n",
    "            batch_features.append(feature[start_index:end_index])\n",
    "        batch_lens = []\n",
    "        for seq_len in seq_lens:\n",
    "            batch_lens.append(seq_len[start_index:end_index])\n",
    "\n",
    "        yield batch_features, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"  \n",
    "        self.data_dir=\"data/processed_reviews\"           # location of reviews data\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.mode=\"train\"                                # train|infer\n",
    "        self.model=\"new\"                                 # old|new\n",
    "        self.lr=1e-4                                     # learning rate\n",
    "        self.num_epochs=20                               # num of epochs \n",
    "        self.batch_size=256                              # batch size\n",
    "        self.hidden_size=200                             # num hidden units for RNN\n",
    "        self.embedding=\"glove\"                           # random|glove\n",
    "        self.emb_size=200                                # num hidden units for embeddings\n",
    "        self.max_grad_norm=5                             # max gradient norm\n",
    "        self.keep_prob=0.9                               # Keep prob for dropout layers\n",
    "        self.num_layers=1                                # number of layers for recurrsion\n",
    "        self.max_input_length=300                        # max number of words per review\n",
    "        self.min_lr=1e-6                                 # minimum learning rate\n",
    "        self.decay_rate=0.96                             # Decay rate for lr per global step (train batch)\n",
    "        self.save_every=10                               # Save the model every <save_every> epochs\n",
    "        self.model_name=\"imdb_model\"                    # Name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model(sess, FLAGS, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a new model or loads old one.\n",
    "    \"\"\"\n",
    "    imdb_model = Model(FLAGS, vocab_size)\n",
    "    imdb_model._build_graph()\n",
    "\n",
    "    if FLAGS.model == 'new':\n",
    "        print ('==> Created a new model.')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    elif FLAGS.model == 'old':\n",
    "        ckpt = tf.train.get_checkpoint_state(\n",
    "            os.path.join(basedir, FLAGS.ckpt_dir))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print(\"==> Restoring old model parameters from %s\" %\n",
    "                ckpt.model_checkpoint_path)\n",
    "            imdb_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print (\"==> No old model to load from so initializing a new one.\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    return imdb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(FLAGS):\n",
    "    \"\"\"\n",
    "    Train a previous or new model.\n",
    "    \"\"\"\n",
    "    # Data paths\n",
    "    vocab_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/vocab.txt')\n",
    "    train_data_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/train.p')\n",
    "    validation_data_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/validation.p')\n",
    "    vocab = Vocab(vocab_path)\n",
    "    FLAGS.num_classes = 2\n",
    "\n",
    "    # Load embeddings (if using GloVe)\n",
    "    if FLAGS.embedding == 'glove':\n",
    "        with open(os.path.join(\n",
    "            basedir, 'data/processed_reviews/embeddings.p'), 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        FLAGS.vocab_size = len(embeddings)\n",
    "\n",
    "    # Start tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Create|reload model\n",
    "        imdb_model = create_model(sess, FLAGS, len(vocab))\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"valid_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"valid_acc\": [],\n",
    "        }\n",
    "\n",
    "        # Store attention score history for few samples\n",
    "        attn_history = {\n",
    "            \"sample_0\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_1\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_2\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_3\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_4\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "        }\n",
    "\n",
    "        # Start training\n",
    "        for train_epoch_num, train_epoch in \\\n",
    "            enumerate(generate_epoch(\n",
    "                train_data_path, FLAGS.num_epochs, FLAGS.batch_size)):\n",
    "\n",
    "            print (\"==> EPOCH:\", train_epoch_num)\n",
    "\n",
    "            for train_batch_num, (batch_features, batch_seq_lens) in \\\n",
    "                enumerate(train_epoch):\n",
    "\n",
    "                batch_reviews, batch_labels = batch_features\n",
    "                batch_review_lens, = batch_seq_lens\n",
    "\n",
    "                # Display shapes once\n",
    "                if (train_epoch_num == 0 and train_batch_num == 0):\n",
    "                    print (\"Reviews: \", np.shape(batch_reviews))\n",
    "                    print (\"Labels: \", np.shape(batch_labels))\n",
    "                    print (\"Review lens: \", np.shape(batch_review_lens))\n",
    "\n",
    "                _, train_logits, train_loss, train_acc, lr, attn_scores = \\\n",
    "                    imdb_model.train(\n",
    "                        sess=sess,\n",
    "                        batch_reviews=batch_reviews,\n",
    "                        batch_labels=batch_labels,\n",
    "                        batch_review_lens=batch_review_lens,\n",
    "                        embeddings=embeddings,\n",
    "                        keep_prob=FLAGS.keep_prob,\n",
    "                        )\n",
    "\n",
    "            for valid_epoch_num, valid_epoch in \\\n",
    "                enumerate(generate_epoch(\n",
    "                    data_path=validation_data_path,\n",
    "                    num_epochs=1,\n",
    "                    batch_size=FLAGS.batch_size,\n",
    "                    )):\n",
    "\n",
    "                for valid_batch_num, (valid_batch_features, valid_batch_seq_lens) in \\\n",
    "                    enumerate(valid_epoch):\n",
    "\n",
    "                    valid_batch_reviews, valid_batch_labels = valid_batch_features\n",
    "                    valid_batch_review_lens, = valid_batch_seq_lens\n",
    "\n",
    "                    valid_logits, valid_loss, valid_acc = imdb_model.eval(\n",
    "                        sess=sess,\n",
    "                        batch_reviews=valid_batch_reviews,\n",
    "                        batch_labels=valid_batch_labels,\n",
    "                        batch_review_lens=valid_batch_review_lens,\n",
    "                        embeddings=embeddings,\n",
    "                        keep_prob=1.0, # no dropout for val|test\n",
    "                        )\n",
    "\n",
    "            print (\"[EPOCH]: %i, [LR]: %.6e, [TRAIN ACC]: %.3f, [VALID ACC]: %.3f \" \\\n",
    "                   \"[TRAIN LOSS]: %.6f, [VALID LOSS]: %.6f\" % (\n",
    "                train_epoch_num, lr, train_acc, valid_acc, train_loss, valid_loss))\n",
    "\n",
    "            # Store the metrics\n",
    "            metrics[\"train_loss\"].append(train_loss)\n",
    "            metrics[\"valid_loss\"].append(valid_loss)\n",
    "            metrics[\"train_acc\"].append(train_acc)\n",
    "            metrics[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "            # Store attn history\n",
    "            for i in range(5):\n",
    "                sample = \"sample_%i\"%i\n",
    "                attn_history[sample][\"review\"] = batch_reviews[i]\n",
    "                attn_history[sample][\"label\"] = batch_labels[i]\n",
    "                attn_history[sample][\"review_len\"] = batch_review_lens[i]\n",
    "                attn_history[sample][\"attn_scores\"].append(attn_scores[i])\n",
    "\n",
    "            # Save the model (maybe)\n",
    "            if ((train_epoch_num == (FLAGS.num_epochs-1)) or\n",
    "            ((train_epoch_num%FLAGS.save_every == 0) and (train_epoch_num>0))):\n",
    "\n",
    "                # Make parents ckpt dir if it does not exist\n",
    "                if not os.path.isdir(os.path.join(basedir, FLAGS.data_dir, 'ckpt')):\n",
    "                    os.makedirs(os.path.join(basedir, FLAGS.data_dir, 'ckpt'))\n",
    "\n",
    "                # Make child ckpt dir for this specific model\n",
    "                if not os.path.isdir(os.path.join(basedir, FLAGS.ckpt_dir)):\n",
    "                    os.makedirs(os.path.join(basedir, FLAGS.ckpt_dir))\n",
    "\n",
    "                checkpoint_path = \\\n",
    "                    os.path.join(\n",
    "                        basedir, FLAGS.ckpt_dir, \"%s.ckpt\" % FLAGS.model_name)\n",
    "\n",
    "                print (\"==> Saving the model.\")\n",
    "                imdb_model.saver.save(sess, checkpoint_path,\n",
    "                                 global_step=imdb_model.global_step)\n",
    "\n",
    "    # Save the metrics\n",
    "    metrics_file = os.path.join(basedir, FLAGS.ckpt_dir, 'metrics.p')\n",
    "    with open(metrics_file, 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "    # Save the attention scores\n",
    "    attn_history_file = os.path.join(basedir, FLAGS.ckpt_dir, 'attn_history.p')\n",
    "    with open(attn_history_file, 'wb') as f:\n",
    "        pickle.dump(attn_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "train(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='9'></a>\n",
    "### **IX. Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.model_name=\"imdb_model\"                     # Name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(FLAGS):\n",
    "    \"\"\"\n",
    "    Plot the loss and accuracy for train|test.\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Load metrics from file\n",
    "    metrics_file = os.path.join(basedir, FLAGS.ckpt_dir, 'metrics.p')\n",
    "    with open(metrics_file, 'rb') as f:\n",
    "        metrics = pickle.load(f)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot results\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(metrics[\"train_acc\"], label='train accuracy')\n",
    "    ax1.plot(metrics[\"valid_acc\"], label='valid accuracy')\n",
    "    ax1.legend(loc=4)\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('train|valid accuracy')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(metrics[\"train_loss\"], label='train loss')\n",
    "    ax2.plot(metrics[\"valid_loss\"], label='valid loss')\n",
    "    ax2.legend(loc=3)\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('train|valid loss')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "# Add model name to ckpt dir\n",
    "FLAGS.ckpt_dir = FLAGS.ckpt_dir + '/%s'%(FLAGS.model_name)\n",
    "plot_metrics(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see a bit of overfitting after ~epoch 7. If you want to achieve the best performance, use all 25,000 training/test samples and include a lot more stringent regularization along with gradient clipping a more rigorous decay. But since just wanted to see some interpretable attention scores, this performance was satifactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='10'></a>\n",
    "### **X. Attention for a Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.data_dir=\"data/processed_reviews\"           # location of reviews data\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.model_name=\"imdb_model\"                     # Name of the model\n",
    "        self.sample_num=2                                # Sample num to view attn plot. [0-4]\n",
    "        self.num_rows=5                                  # Number of rows to show in attn visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_attn(input_sentence, attentions, num_rows, save_loc=None):\n",
    "    \"\"\"\n",
    "    Plot the attention scores.\n",
    "    Args:\n",
    "        input_sentence: input sentence (tokens) without <pad>\n",
    "        attentions: attention scores for each token in input_sentence\n",
    "        num_rows: how many rows you want the figure to have (we will add 1)\n",
    "        save_loc: fig will be saved to this location\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine how many words per row\n",
    "    words_per_row = (len(input_sentence.split(' '))//num_rows)\n",
    "\n",
    "    # Use one extra row in case of remained for quotient above\n",
    "    fig, axes = plt.subplots(nrows=num_rows+1, ncols=1, figsize=(20, 10))\n",
    "    for row_num, ax in enumerate(axes.flat):\n",
    "\n",
    "        # Isolate pertinent part of sentence and attention scores\n",
    "        start_index = row_num*words_per_row\n",
    "        end_index = (row_num*words_per_row)+words_per_row\n",
    "        _input_sentence = \\\n",
    "            input_sentence.split(' ')[start_index:end_index]\n",
    "        _attentions = np.reshape(\n",
    "            attentions[0, start_index:end_index],\n",
    "            (1, len(attentions[0, start_index:end_index]))\n",
    "            )\n",
    "\n",
    "        # Plot attn scores (constrained to [0.9, 1] for emphasis)\n",
    "        im = ax.imshow(_attentions, cmap='Blues', vmin=0.9, vmax=1)\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels(\n",
    "            [''] + _input_sentence,\n",
    "            rotation=90,\n",
    "            minor=False,\n",
    "            )\n",
    "        ax.set_yticklabels([''])\n",
    "\n",
    "        # Set x tick to top\n",
    "        ax.xaxis.set_ticks_position('top')\n",
    "        ax.tick_params(axis='x', colors='black')\n",
    "\n",
    "        # Show corresponding words at the ticks\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # Add color bar\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "\n",
    "    # display color bar\n",
    "    cb = fig.colorbar(im, cax=cbar)\n",
    "    cb.set_ticks([]) # clean color bar\n",
    "\n",
    "    if save_loc is None:\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Save the plot\n",
    "        fig.savefig(save_loc, dpi=fig.dpi, bbox_inches='tight') # dpi=fig.dpi for high res. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_attn_inputs(FLAGS, review, review_len, raw_attn_scores):\n",
    "    \"\"\"\n",
    "    Return the inputs needed to\n",
    "    plot the attn scores. These include\n",
    "    input_sentence and attn_scores.\n",
    "    Args:\n",
    "        FLAGS: parameters\n",
    "        review: list of ids\n",
    "        review_len: len of the relevant review\n",
    "    Return:\n",
    "        input_sentence: inputs as tokens (words) on len <review_len>\n",
    "        plot_attn_scoes: (1, review_len) shaped scores\n",
    "    \"\"\"\n",
    "\n",
    "    review_len = 300\n",
    "\n",
    "    # Data paths\n",
    "    vocab_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/vocab.txt')\n",
    "    vocab = Vocab(vocab_path)\n",
    "\n",
    "    review = review[:review_len]\n",
    "    attn_scores = raw_attn_scores[:review_len]\n",
    "\n",
    "    # Process input_sentence\n",
    "    input_sentence = ' '.join([item for item in ids_to_tokens(review, vocab)])\n",
    "\n",
    "    # Process attn scores (normalize scores between [0,1])\n",
    "    min_attn_score = min(attn_scores)\n",
    "    max_attn_score = max(attn_scores)\n",
    "    normalized_attn_scores = ((attn_scores - min_attn_score) / \\\n",
    "        (max_attn_score - min_attn_score))\n",
    "\n",
    "    # Reshape attn scores for plotting\n",
    "    plot_attn_scores = np.zeros((1, review_len))\n",
    "    for i, score in enumerate(normalized_attn_scores):\n",
    "        plot_attn_scores[0, i] = score\n",
    "\n",
    "    return input_sentence, plot_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_sample_attn(FLAGS):\n",
    "    \"\"\"\n",
    "    Use plot_attn from utils.py to visualize\n",
    "    the attention scores for a particular\n",
    "    sample FLAGS.sample_num.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the attn history\n",
    "    attn_history_path = os.path.join(\n",
    "        basedir, FLAGS.ckpt_dir, 'attn_history.p')\n",
    "    with open(attn_history_path, 'rb') as f:\n",
    "        attn_history = pickle.load(f)\n",
    "\n",
    "    # Process the history to get the right sample\n",
    "    sample = \"sample_%i\" % (FLAGS.sample_num)\n",
    "    review_len = attn_history[sample][\"review_len\"]\n",
    "    review = attn_history[sample][\"review\"]\n",
    "    label = attn_history[sample][\"label\"]\n",
    "    attn_scores = attn_history[sample][\"attn_scores\"][-1]\n",
    "\n",
    "    input_sentence, plot_attn_scores = get_attn_inputs(\n",
    "        FLAGS=FLAGS,\n",
    "        review=review,\n",
    "        review_len=review_len,\n",
    "        raw_attn_scores=attn_scores,\n",
    "        )\n",
    "\n",
    "    # Plot and save fig\n",
    "    fig_name = \"sample_%i\" % (FLAGS.sample_num)\n",
    "    save_loc = os.path.join(basedir, FLAGS.ckpt_dir, fig_name)\n",
    "    plot_attn(\n",
    "        input_sentence=input_sentence,\n",
    "        attentions=plot_attn_scores,\n",
    "        num_rows=FLAGS.num_rows,\n",
    "        save_loc=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "# Add model name to ckpt dir\n",
    "FLAGS.ckpt_dir = FLAGS.ckpt_dir + '/%s'%(FLAGS.model_name)\n",
    "process_sample_attn(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='11'></a>\n",
    "### **XI. Attentional History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the cell below and click the play arrow to view the historical attentional scores and learning progression for the same sample as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"950\" height=\"500\" autoplay loop>\n",
    "  <source src=\"images/sample_attention_history.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see how the model starts with equal attention distribution to all words and then starts to learn which words are important in order to predict the sentiment. The first sanity check is that the PAD tokens receive almost no attention. And then we start to see focus on really strong/influential words. Check out few of the other reviews as well and you will see that almost always, there is quite a bit of attention at the ends of reviews which is where people give a concluding statement that summarizes their sentiment succinctly.\n",
    "\n",
    "We used the sentiment analysis task in order to see how simple it is to add an attentional interface to a model. Though we were able to receive some meaningful attention scores for this task, it is not a trivial task to interpret even with attention. Most reviews (discarding the extremes) will usually talk about the plot for most of the review so it is pretty cool that we can use attention to pick up on the brief moments of sentiment. An interesting extension of this implementation would be to only use the extreme reviews, where strong emotions surface throughout the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='12'></a>\n",
    "### **XII. Attentional Interface Variants**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is one of the basic forms of attention and we can already start to see some interpretability. This is currently a very active area of research where there are developments in different types of tasks using attention (VQA, translation, etc.), different types of attention, and better attentional interface architecture. In this post so far, we have seen how attentional processing can be unique to the task. With the translation task, the attentional interface is applied to each input word and a summarized context is made for each time step. This is because translation is not always a one-to-one task and a word may depend on several words in the target language for the correct translation. However, for our sentiment analysis implementation, it wouldn't make sense to have a summarizing vector for each input because our output is a binary sentiment. Similarly, there are also processing variants for tasks such as question-answering (ex. pointer-based attention to specific words in the input) and image based attention (ex. focusing on different parts of the represented image).\n",
    "\n",
    "We can also vary the architecture of the attention mechanism as well. This too is a large, open field of research and we may cover some of the interesting variants in future articles. Here we'll introduce one of the architecture variants that is relevant to our sentiment analysis (or any NLU) task.\n",
    "\n",
    "In the attentional interface we implemented, you may notice that we completely disregard the order of the input review. We processes all of the words in the review but when we read a review and try to determine the sentiment, it is important to have context for what we have already read. A small tweak in the attentional interface can help us overcome this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](images/attngru.jpg)\n",
    "<center><b> Figure 6 </b>[Attention GRU.](http://arxiv.org/abs/1603.01417)</center>\n",
    "<center>Credit: Goku Mohandas</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to incorporate order of the input into our attentional interface, all we have to do is first, compute the attention scores for each token in the input. Then we feed in one token at a time, with the scalar (or vector) attention score in place of the update gate in the GRU. We do this for all tokens in the input and we take the final (relevant output) or all of the outputs, which of course depends on our task and model architecture. This tweak allows us to incorporate the order of the input along attentional scores, giving us both interpretability and a logical architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='13'></a>\n",
    "### **XIII. Caveats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you may notice, this increased interpretability does come with an increased computation cost. This can be a major factor when using these models for production systems, so it is really important to understand the types of tasks for which these interfaces are best-suited. For the specific task of binary sentiment analysis, we can get similar performance with just a bag-of-words model but the model ultimately depends on the performance you are looking for and the computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='14'></a>\n",
    "### **XIV. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we start to develop increasingly deep models, it will be important to maintain a high level of interpretability to help us manage complexity. This becomes more valuable as AI starts to have a greater impact on our everyday lives and we start integrating it everywhere. Attentional interfaces are just the beginning for this field in terms of having transparent models and we need to keep pushing for increased interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='15'></a>\n",
    "### **XV. References**\n",
    "1. Desimone, Robert, and John Duncan. Neural mechanisms of selective visual attention. Annual review of neuroscience 18.1 (1995): 193-222.\n",
    "2. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel: “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, 2015; [http://arxiv.org/abs/1502.03044 arXiv:1502.03044].\n",
    "3. Dzmitry Bahdanau, Kyunghyun Cho: “Neural Machine Translation by Jointly Learning to Align and Translate”, 2014; [http://arxiv.org/abs/1409.0473 arXiv:1409.0473].\n",
    "4. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi: “Bidirectional Attention Flow for Machine Comprehension”, 2016; [http://arxiv.org/abs/1611.01603 arXiv:1611.01603].\n",
    "5. William Chan, Navdeep Jaitly, Quoc V. Le: “Listen, Attend and Spell”, 2015; [http://arxiv.org/abs/1508.01211 arXiv:1508.01211]\n",
    "6. Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman: “Teaching Machines to Read and Comprehend”, 2015; [http://arxiv.org/abs/1506.03340 arXiv:1506.03340].\n",
    "7. Dzmitry Bahdanau, Kyunghyun Cho: “Neural Machine Translation by Jointly Learning to Align and Translate”, 2014; [http://arxiv.org/abs/1409.0473 arXiv:1409.0473].\n",
    "8. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "9. Diederik P. Kingma: “Adam: A Method for Stochastic Optimization”, 2014; [http://arxiv.org/abs/1412.6980 arXiv:1412.6980].\n",
    "10. Caiming Xiong, Stephen Merity: “Dynamic Memory Networks for Visual and Textual Question Answering”, 2016; [http://arxiv.org/abs/1603.01417 arXiv:1603.01417]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='16'></a>\n",
    "### **XVI. Author Bio:**\n",
    "Goku Mohandas is an artificial intelligence (AI) researcher in Silicon Valley with a focus on using deep learning for natural language tasks. His interests include research on AI for intelligent search and question answering augmented by attentional and memory-based interfaces. He also strongly believes in the democratization of AI with a focus on interpretability and transparency. Previous work includes working on the intersection of AI and biotechnology at the Johns Hopkins University Applied Physics Laboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Twitter: [@GokuMohandas](https://twitter.com/gokumohandas)\n",
    "* Github: [ajarai](https://github.com/ajarai)\n",
    "* Blog: [The Neural Perspective](http://www.theneuralperspective.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
